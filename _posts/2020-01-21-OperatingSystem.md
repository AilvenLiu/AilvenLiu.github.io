---
layout:     post
title:      操作系统常见问题合集          
subtitle:   操作系统               
date:       2022-04-21
author:     OUC_LiuX
header-img: img/wallpic02.jpg
catalog: true
tags:          
    - CS basis
---      

## Backtrace 是怎么产生的           
backtrace 的实现依赖于栈指针（fp寄存器），FP寄存器保存的是上一个函数的栈底，然后一层层出栈即可得到函数调用过程。在gcc编译过程中任何非零的优化等级（-On参数）或加入了栈指针优化参数 `-fomit-frame-pointer` 后多将不能正确得到程序栈信息。             


## CAS 是什么东西         
CAS(Compare And Swap) 是一种无锁实现原子性数据更新的操作。具体而言，CAS 机制中有三个核心参数：V，A，B。其中，V 是数据对应的内存地址，A 是 V 地址对应的数据旧的期望值，B 是数据新的期望值。当线程要通过 CAS 机制修改主内存 V 中的值，则先将旧的数据期望值 A 读出到工作内存，在将新的期望值 B 写入到主内存前，先比较当前内存 V 中的数据是否等于 A，如果是，证明该内存处存储的数据没有被修改过，将 B 写入，并返回 true；否则，不作任何操作并返回 false。            

CAS 的全过程在操作系统中只由一条不可分割的 `cmpxchgl` 硬件汇编指令实现，指令执行不可中断，是直接对 CPU 进行操作，从而在硬件层面保证了其原子性。但是，单单这一条指令在多核状态下依旧不能保证原子性。多和状态下需要在该指令前添加一条 `lock` 指令，该指令会锁定总线（阻止CPU通过总线读写内存，代价过高，当数据位于 L1～L3 Cache，且数据长度不超过 cache line ，则只会锁住缓存行）或 CPU 的缓存行从而保证操作的原子性。         

进一步地，如果发现数据已经被其他线程更改，就开始轮询，不断重试，直到成功修改。就是 **乐观锁** 的一种实现。         

#### CAS 的优缺点：         
优点：     
1. 保证变量操作的原子性。          
2. 并发量不是很高的情况下，使用 CAS 机制比使用锁机制效率更高。          
3. 在线程对共享资源占用时间较短的情况下，使用 CAS 机制效率更高。           

缺点：       
1. ABA 问题：假设两个线程，线程1 和线程2， 按照顺序有如下操作：         
   1） 线程1 读取内存中的数据为 A；          
   2） 线程2 修改内存中的数据为 B；        
   3） 线程2 修改内存中的数据为 A;       
   4） 线程1 堆内存中的数据执行 CAS 操作。           
   显然，内存中的数据被线程2 修改过，但是线程1 在修改前比较读出时和当前的值相等， CAS 是可以成功的。          
   该问题带来的典型隐患如 **栈顶问题** ，一个栈的栈顶经过两次(或多次)变化又恢复了原值，但是栈可能已发生了变化。            

   解决 ABA 问题比较有效的方案是引入版本号，内存中的值每一次发生变化，版本号都 +1， 进行 CAS 操作时不仅比较内存中的值，也比较版本号是否相等，只有当二者都未变化时，CAS 才能成功执行。           

2. 高竞争下的开销问题。实际这属于使用 CAS 实现的乐观锁的问题，在并发冲突概率大的高竞争环境下，如果CAS一直失败，会一直重试，CPU开销较大。针对这个问题的一个思路是引入退出机制，如重试次数超过一定阈值后失败退出。当然，更重要的是避免在高竞争环境下使用乐观锁。          

3. CAS 只能保证变量操作的原子性，无法保证整个代码块的线程安全。        


#### 什么是原子性       
上文提到了原子性。原子性是指一个或者多个操作在 CPU 执行的过程中不被中断的特性，要么完整执行成功，要么退回到初始状态，，不能只执行到一半。         


## 乐观锁，悲观锁，互斥锁，自旋锁，读写锁是些什么锁           

* **乐观锁：**       
  乐观锁或悲观锁是一种编程思想，并不是真正的实现。乐观锁实际上是不加锁，是一种无锁编程方式。乐观锁认为，其他线程争抢共享变量的概率相对小，所以更新数据的时候不会对共享变量加锁，但是在正式将数据写入内存之前会检查该数据是否被其他线程修改过（值或版本号）。如果未被修改过，则写入内存完成修改，否则就重试直到成功为止。可以认为，CAS + 轮询 就是乐观锁的一种实现。**乐观锁适用于冲突概率较低的情况，适用于读操作较多的应用类型**，这样可以提高吞吐量。          
  一个典型的例子是在线文档，由于发生冲突的概率较低，所以先允许用户进行编辑，但是浏览器下载文档时会记录下服务端返回的文档版本号；当用户提交修改时，发送给服务端的请求会带上原始文档的版本号，服务器收到后将它与当前版本号进行比较。版本号一致则修改成功，否则修改失败。         

* **悲观锁：**                  
  悲观锁则认为目标资源被其他线程竞争的概率相对较大，因此操作共享资源的时候总是对资源加锁，直到操作结束后释放。加锁的资源既可以是变量，也可以是代码块。互斥所、自旋锁和读写锁等等都是悲观锁的具体实现。           

* **互斥锁：**             
  互斥锁和自旋锁是最基本的两种加锁方式，是其他较高级锁的基础。当已经有一个线程加锁后，其他线程加锁则就会失败，互斥锁和自旋锁的区别就在于对于加锁失败的线程的处理方式上。          
  互斥锁加锁失败后，当前线程会 **释放 CPU 资源** 给其他线程，自己进入阻塞状态，从用户态陷入到内核态。当锁被释放后，内核负责将该线程重新唤醒，由阻塞状态恢复为就绪状态，从内核态恢复到用户态。        
  互斥所存在一定的性能开销成本。具体而言，线程互斥锁加锁失败后存在从用户态（就绪）到内核态（阻塞）和从内核态返回用户态的两次转换，相应地需要保存和恢复线程相关的寄存器和栈，也就是线程的上下文切换。于是如果**被锁住的代码的执行时间较短，甚至比线程上下文切换的时间还短，就不应该使用互斥锁。**       

* **自旋锁：**             
  自旋锁则不存在线程上线文切换的问题。一般的，加锁过程分为两步：查看锁的状态，如果空闲则将锁设为当前线程持有。自旋锁通过 CAS 函数实现这一过程。CAS 将这两部操作合并为一条不可分的硬件指令，在硬件层面保证了加锁操作的原子性。           
  使用自旋锁的时候，如果发生多线程竞争锁的情况，加锁失败的线程会进入 **“忙等待”** 状态，一直自旋，利用 CPU 周期，直到锁可用。忙等待可以通过 while 循环实现，但是用 CPU 级别的 PAUSE 指令更加节能。          
  自旋锁的系统开销较小，在多核系统中一般不主动产生线程上下文切换。但如果被锁住的代码执行时间较长，自旋状态的线程也会长时间占用 CPU 资源。与互斥锁正相反，**自旋锁适用于代码执行时间较短的场景。**         
  特别的，在单核单线程系统中，自旋锁是不适用的，即使代码里实现了自旋锁，其也会被编译器优化掉。这是因为当一个线程进入自旋状态，证明其正在等待某一资源。在单核单线程 CPU 系统中，一旦发生了这种自旋锁，那么当前持有资源的线程也会由于无法获得 CPU 而一直等待。这样，占用 CPU 的线程等待另一个线程释放资源，占用资源的线程等待另一个线程释放 CPU ，从而导致程序进入死锁。          
  

* **读写锁：**               
  互斥锁和共享锁是两种最基本的加锁方式，更高级的锁会选择其中一个进行升级，比如读写锁既可以选择互斥锁实现，也可以基于自旋锁实现。读写锁从字面意思我们也可以知道，它由 *读锁* 和 *写锁* 两部分构成，如果只读取共享资源用 *读锁* 加锁，如果要修改共享资源则用 *写锁* 加锁。             
  所以，**读写锁适用于能明确区分读操作和写操作的场景**。              
  读写锁的工作原理是：              
  * 当 *写锁* 没有被线程持有，多个线程可以并发地持有 *读锁* ，这大大提高了共享资源的访问效率。     
  * 当 *写锁* 被线程持有，读线程获取 *读锁* 的操作会被阻塞，其他线程获取 *写锁* 的操作也会被阻塞。     
  
  从而得出，*写锁* 是独占锁，任何时候只能被一个线程持有，可以有互斥锁或自旋锁等实现；而 *读锁* 是共享锁，可以同时被多个线程持有。           
  显然，读写锁在**读多写少的场景下更能发挥出优势**。          

  根据优先级的不同，读写锁可以实现为 *读优先锁* 和 *写优先锁* 。          
  读优先锁期望的是，读锁能被更多的线程持有，以便提高读线程的并发性，它的工作方式是：当读线程 A 先持有了读锁，写线程 B 在获取写锁的时候，会被阻塞，并且在阻塞过程中，后续来的读线程 C 仍然可以成功获取读锁，最后直到读线程 A 和 C 释放读锁后，写线程 B 才可以成功获取写锁。             
  写优先锁希望优先服务写线程，其工作方式是，当读线程 A 先持有了读锁，写线程 B 在获取写锁的时候，会被阻塞，并且在阻塞过程中，后续来的读线程 C 获取读锁时会失败，于是读线程 C 将被阻塞在获取读锁的操作，这样只要读线程 A 释放读锁后，写线程 B 就可以成功获取写锁。            

  读优先锁对于读线程的并发性更好，但如果一直有读线程获取读锁，那么写线程永远无法获取写锁，就产生了写线程饥饿的现象。同理，写优先锁情况下，如果一直有写线程获取写锁，读线程也会产生饥饿现象。对于线程饥饿问题的一种解决方式是“读写公平锁”：              
  读写公平锁的一种简单实现是，维护一个队列，所有获取锁的线程按顺序入队列，不管是读线程还是写线程，按照先进先出的原则出队列加锁即可。这样读线程仍然可以并发，也不会出现饥饿现象。          
 

## Debug 和 Release                
Debug 版本和 Release 版本其实就是 C/C++ 编译器优化级别的不同。gcc/g++ 编译器的有如下几种优化级别：              
* O0： 默认级别，不开启优化项，方便调试，也就是 BEBUG 模式。           
* Og： 开启部分不影响调试信息的优化项。        
* O1： 较保守的优化。         
* O2： 常见的 Release 级别，打开几乎全部优化项。           
* Os： 比 O2 更保守一些。           
* O3： 更加激进。             
* Ofast： 不严格遵循标准，在 O3 的基础上开启一些可能导致不符合 IEEE 浮点数标准的优化项。      

DEBUG 是调试版本，编译的结果通常包含调试信息，不进行任何优化，因此代码运行相对较慢。RELEASE 是发布版本，不携带调试信息，比如断点，同时编译器对代码进行很多优化，是代码更小，速度更快。RELEASE 因此也比 DEBUG 需要更长的编译时间。               
DEBUG 模式下，申请内存时会多分配一些内存空间，分布在申请内存块的前后，用于存放调试信息；RELEASE 则不会。对于未初始化变量，DEBUG 默认将其初始化，RELEASE 则不会。DEBUG 下可以使用断言，RELEASE 则会直接无视掉断言语句。 DEBUG 以 32 字节为单位分配内存，例如当申请 24 字节内存时，RELEASE 是正常的 24 字节，而 DEBUG 则会多申请 8 个字节；所以有些数组越界问题在 DEBUG 模式下可以安全运行，RELEASE 模式下则会出现问题。DEBUG 模式下记录了可以正常使用 gdb 去 debug，但是 RELEASE 可能会将用于记录程序运行信息的符号表 优化掉，导致无法直接 gdb。如果需要在 release 版本 gdb 的话，可以使用 objcopy 生成一个 debug 信息，在 gdb 的时候，先加载符号信息 `file xxx.debug`，然后再进行调试。         


#### Core Dump 是怎么产生的          
上文提到了 core dump 。当程序运行的过程中异常终止或崩溃，操作系统会将程序当时的内存状态记录下来，保存在一个文件中，这种行为就叫做Core Dump。 使用命令 `gdb program core` 来查看 core 文件，其中 program 为可执行程序名，core 为生成的 core 文件名。             


## 零拷贝            

网络编程中，服务端面临客户端请求时，会发生如下系统调用：        
```c++
// 伪代码              
File::read(file, buf, len);
Socket::send(socket, bufm len);
```
也即将数据从外部存储设备读入用户空间，然后通过外部发送设备（网卡）发送出去。在没有使用任何优化技术的情况下，这一过程需要进行四次数据拷贝，进行四次上下文切换，如下图所示：           
<div align=center><img src="https://raw.githubusercontent.com/OUCliuxiang/OUCliuxiang.github.io/master/img/CSbasis/OS01.jpg"></div>         

其中，四次拷贝：             
* 物理设备 <--> 内存：             
  * CPU 负责将数据从硬盘拷贝到内核空间的 Page Cache 中。            
  * CPU 负责将数据从内核空间的 Socket 缓冲区拷贝到网络设备中。            
* 内存内部拷贝：             
  * CPU 负责将数据从内核空间的 Page Cache 拷贝到用户控件缓冲区。             
  * CPU 负责将数据从用户空间缓冲区拷贝到内核空间 Socket 缓冲区。       

线程的四次上下文切换：             
1. read 系统调用时，从用户态切换到内核态。            
2. read 系统调用完毕，从内核态切换回用户态。             
3. write 系统调用时，从用户态陷入到内核态。              
4. write 系统调用完毕，从内核态恢复到用户态。            

CPU 直接控制硬盘数据 IO 需要 CPU 不断轮询当前字节数据 IO 是否完成，但由于 CPU 处理速度较高，而设备间 IO 速度较慢，这种方式往往导致 CPU 大多数时间处于忙等待状态，效率较低。上下文切换的成本也不小：一次切换需要耗时几十纳秒到几微秒，虽然时间看上去很短，但是在高并发的场景下，这类时间容易被累积和放大，从而影响系统的性能。               

现代计算机通过 **DMA** (Direct Memory Access，直接内存访问) 机制实现设备间 IO，也即通过主板上的 DMAC (DMA Controller，DMA 控制器) 控制数据 IO。DMAC 作为一个协控制器接管线程负责设备间 IO，在此期间 CPU 资源被释放，并在 IO 完成后，通知 CPU 重新接管线程。但是**设备内部的数据拷贝还需要 CPU 来亲力亲为**。如下图：               
<div align=center><img src="https://raw.githubusercontent.com/OUCliuxiang/OUCliuxiang.github.io/master/img/CSbasis/OS02.png"></div>         


DMA 机制是实现零拷贝的的基础之一。零拷贝并不是没有拷贝数据，而是减少用户态/内核态的切换次数以及 CPU 负责拷贝的次数，主要是用来解决操作系统在处理 I/O 操作时，频繁复制数据的问题。关于零拷贝主要技术有 `mmap+write`、`sendfile` 和 `splice` 等几种方式。          

**mmap + write**          
`mmap` 依据虚拟内存空间而实现。现代操作系统中，进程（用户态和内核态）无法直接访问真实的物理内存地址，进程能看到的所有地址组成的空间，实际是虚拟内存空间。而虚拟内存空间和物理内存空间存在映射关系，进程通过虚拟内存和虚拟内存与物理内存之间的映射关系间接访问物理内存。是用虚拟内存空间有两种好处：          
* 虚拟内存空间可以远大于物理内存空间           
* 多个虚拟内存地址可以指向同一个物理地址             

`mmap` 正是基于后者而实现。它将**用户空间和内核空间的虚拟地址映射到同一个物理地址**，从而在读数据的时候，只需要 DMA 将磁盘数据拷贝到内核空间即可，而不需要继续从内核空间拷贝到用户空间。通过将用户空间的缓冲区映射到和内核读空间缓冲区相同的物理地址，再次向外部设备写入数据的时候，则直接在内核空间，由内核读缓冲区拷贝到 socket 缓冲区。其具体流程如下：             
<div align=center><img src="https://raw.githubusercontent.com/OUCliuxiang/OUCliuxiang.github.io/master/img/CSbasis/OS03.jpg"></div>         

* 用户进程通过 mmap 方法箱操作系统内核发起 IO 调用，上下文从**用户态切换到内核态**。         
* CPU 通过 DMAC 将数据从存储设备拷贝到内核缓冲区。                
* IO 完成，mmap 方法返回，上下文从**内核态切换回用户态**。             
* 用户进程通过 write 方法箱操作系统内核发起 IO 调用，上下文从**用户态切换到内核态**。          
* CPU 将内核读缓冲区内的数据拷贝到 socket 缓冲区。            
* CPU 利用 DMAC 将数据从 socket 缓冲区拷贝到网卡。           
* write 调用结束，上下文从**内核态切换回用户态**。         

可以发现，`mmap+write` 方法实现的零拷贝，共发生了三次数据拷贝，包括 2 次设备间的 DMA 拷贝和一次内核空间中的 CPU 拷贝。但依然发生了四次用户空间和内核空间的上下文切换。          

发现，从 mmap 调用结束，到发起 write 之间，数据跟本不经过用户空间，而上下文却仍需要切换回用户态再切回到内核态，产生了不小的性能浪费。那么，能不能省去这两次上下文切换，让上下文一直停留在内核空间，直到 write 结束再返回用户空间呢？方法就是 `sendfile`。              

**sendfile**               
`sendfile` 是 Linux2.1 内核引入的一个系统调用，API 为：          
```
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```         

`sendfile` 方法将文件描述符 in_fd 中，自 offset 位置（默认为 0 ）开始的 count 长度的数据传输到文件描述符 out_fd 中。其返回值为传输的文件大小，传输成功则返回值与count相等。这个操作实现在操作系用内核中，避免了内核空间和用户空间的拷贝。同时，sendfile 将 `read(); write();` 两次系统调用替换为了 `sendfile()` 一次系统调用，于是线程的上下文切换也就减少到了两次。其具体流程如下：                        
<div align=center><img src="https://raw.githubusercontent.com/OUCliuxiang/OUCliuxiang.github.io/master/img/CSbasis/OS04.jpg"></div>         

* 用户进程发起 sendfile 系统调用，上下文从**用户态切换到内核态**。           
* CPU 通过 DMAC 将数据从外部设备拷贝到内核缓冲区。             
* CPU 将内核读缓冲区的数据拷贝到 socket 缓冲区。            
* CPU 通过 DMAC 将数据从 socket 缓冲区拷贝到网卡。           
* sendfile 调用结束，上下文从**内核态切换回用户态**。          

发现，sendfile 实现的零拷贝发生了三次数据拷贝和两次上下文切换。其中三次数据拷贝包括两次 DMA 和一次 CPU 拷贝。那么，能不能再进一步，不要 *内核读缓冲区--> socket 缓冲区* 这一步骤，将 CPU 拷贝的次数降低至 0 次，直接将数据从内核读缓冲区拷贝入网卡设备呢？可以的。            


**sendfile + DMA scatter/gather 实现的零拷贝**              
实际上，上述流程是原始的 sendfile ，在 Linux2.4 内核版本之后，引入了 SG-DMA 技术（ 带 scatter/gather 的 DMA），对 sendfile 进行了升级。SG-DMA 技术可以直接从内核空间缓冲区中将数据读取到网卡。使用这个特点搞零拷贝，即还可以多省去一次CPU拷贝。其具体流程如下：                
<div align=center><img src="https://raw.githubusercontent.com/OUCliuxiang/OUCliuxiang.github.io/master/img/CSbasis/OS05.jpg"></div>         

* 用户进程发起 sendfile 系统调用，**上下文从用户态切换到内核态**。           
* CPU 通过 DMAC 将数据从外部存储设备拷贝到内核缓冲区。                  
* CPU 把内核缓冲区的文件描述符信息（包括内核缓冲区的内存地址和偏移量）发送到 socket 缓冲区。       
* CPU 控制 DMAC，根据文件描述符信息，直接将数据从内核读缓冲区拷贝到网卡。          
* sendfile 调用结束，**上下文从内核态切换回用户态**。           

发现，sendfile + DMA scatter/gather 实现的零拷贝发生了两次上下文切换和两次设备间 DMA 拷贝，而 CPU 拷贝的次数为 0。这是真正的零拷贝技术，全程没有 CPU 负责的数据拷贝，所有数据都是通过 DMA 进行传输的。             

**！但是！：** 基于 SG-DMA 技术实现的零拷贝，必须要网卡支持 scatter-gather 特性，否则依然是 普通 sendfile 机制。可以使用如下命令查看网卡的 scatter-gather 支持：       
```bash 
ethtool -k 网卡名 | grep scatter-gather 
```

很可惜，我自己的机器的网卡并不支持 scatter-gather 。            
<div align=center><img src="https://raw.githubusercontent.com/OUCliuxiang/OUCliuxiang.github.io/master/img/CSbasis/OS06.jpg"></div>       



**零拷贝的缺陷**           
1. 由于数据不经过用户空间，程序无法对数据进行修改。也就是说，零拷贝只适用于将数据不加处理地从硬盘发送到网卡的情况。             
2. 零拷贝依赖于

#### Page Cache 是什么             
上文提到了页缓存 Page Cache，作简要介绍。          

由于读写硬盘的速度远远慢于读写内存的速度，所以我们应该尽量将 *读磁盘* 操作替换为 *读内存* 。但是，内存空间远比磁盘要小，内存注定只能拷贝磁盘里的一小部分数据。那应该选择哪些磁盘数据拷贝到内存呢？

程序运行的时候，具有 *局部性* 的特征 ，所以通常，刚被访问的数据在短时间内再次被访问的概率很高，于是我们可以用 PageCache 来**缓存最近被访问的数据**，当空间不足时淘汰最久未被访问的缓存。这样一来，发生数据读取系统调用的时候，程序优先在 PageCache 找，如果数据存在则可以直接返回；如果没有，则从磁盘中读取，然后缓存 PageCache 中。          

Linux 内核会以页大小（4KB）为单位，将文件划分为多数据块。当用户对文件中的某个数据块进行读写操作时，内核首先会申请一个内存页（页缓存）与文件中的数据块进行绑定。用户读写数据实际是对文件的 `页缓存` 进行读写：            
* 当从文件中读取数据时，如果要读取的数据所在的页缓存已经存在，那么就直接把页缓存的数据拷贝给用户即可。否则，内核首先会申请一个空闲的内存页（页缓存），然后从文件中读取数据到页缓存，并且把页缓存的数据拷贝给用户。              
* 当向文件中写入数据时，如果要写入的数据所在的页缓存已经存在，那么直接把新数据写入到页缓存即可。否则，内核首先会申请一个空闲的内存页（页缓存），然后从文件中读取数据到页缓存，并且把新数据写入到页缓存中。对于被修改的页缓存，内核会周期性地把这些页缓存刷新到文件中。              

**page cache 的回收机制**            
> 当物理内存空间紧张时，内核需要把已经缓存着的内容选择一部分清除。选择的策略通常是基于最近最少使用（LRU）而改进的 LRU/n 算法，也即维护 n 个最近最少使用链表，将最近被访问的页缓存插入链表尾，而链表头存储的就是最近最少使用的页缓存，可以被回收。           


另一点是，读取磁盘数据的时候，需要找到数据所在的位置。对于机械磁盘来说，就是通过磁头旋转到数据所在的扇区，再读取数据。但是旋转磁头这个物理动作是非常耗时的，为了降低它的影响，PageCache 使用了 *预读功能* ：假设 read 方法每次只会读 32 KB 的字节，虽然 read 刚开始只会读 0 ～ 32 KB 的字节，但内核会把其后面的 32～64 KB 也读取到 PageCache，这样后面读取 32～64 KB 的成本就很低。如果在 32～64 KB 淘汰出 PageCache 前，进程读取到它了，收益就非常大。

综上，PageCache 的优点主要是两个：         
1. 缓存最近被访问的数据；          
2. 预读功能；         

这两个做法，大大提高了读写磁盘的性能。           

      

